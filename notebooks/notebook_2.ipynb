{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e6caff5",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcfc5ed",
   "metadata": {},
   "source": [
    "This notebook focuses on extracting meaningful subgraphs from Panama Papers.\n",
    "The goal is to isolate the most structurally and socially significant parts of the network — the nodes and relationships that reveal how offshore systems are organized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ad3823",
   "metadata": {},
   "source": [
    "### Mini-Dense Subgraph Extraction – Initial Node and Relationship Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a5f09a",
   "metadata": {},
   "source": [
    "#### Library Imports and Basic Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cba867c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "DATA_DIR = \"../Dataset\"          # dossier d'origine\n",
    "OUTPUT_DIR = \"./extracts_mini_dense\"  # dossier de sortie\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "REL_FILE = os.path.join(DATA_DIR, \"relationships.csv\")\n",
    "NODE_FILES = [\n",
    "    os.path.join(DATA_DIR, \"nodes-entities.csv\"),\n",
    "    os.path.join(DATA_DIR, \"nodes-intermediaries.csv\"),\n",
    "    os.path.join(DATA_DIR, \"nodes-officers.csv\"),\n",
    "    os.path.join(DATA_DIR, \"nodes-addresses.csv\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5ec385",
   "metadata": {},
   "source": [
    "#### Load Relationships\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f976edb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Relationships loaded (3,339,267 rows)\n"
     ]
    }
   ],
   "source": [
    "relationships = pd.read_csv(\n",
    "    REL_FILE, \n",
    "    usecols=[\"node_id_start\", \"node_id_end\", \"rel_type\"],\n",
    "    low_memory=False\n",
    ")\n",
    "print(f\"✅ Relationships loaded ({len(relationships):,} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b0ee55",
   "metadata": {},
   "source": [
    "#### Identify Pivots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d13da6a",
   "metadata": {},
   "source": [
    "Pivots are highly connected or central nodes that act as key connectors within the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "414f7e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Calculating degree (number of connections)...\n",
      "✅ 339 pivots identified (degree > 500)\n"
     ]
    }
   ],
   "source": [
    "print(\"🔍 Calculating degree (number of connections)...\")\n",
    "\n",
    "deg = pd.concat([\n",
    "    relationships[\"node_id_start\"],\n",
    "    relationships[\"node_id_end\"]\n",
    "]).value_counts().reset_index()\n",
    "deg.columns = [\"node_id\", \"degree\"]\n",
    "\n",
    "# 🔸 Adjust threshold here to target ~500–1000 nodes\n",
    "threshold = 500  \n",
    "top_nodes = deg[deg[\"degree\"] > threshold]\n",
    "print(f\"✅ {len(top_nodes):,} pivots identified (degree > {threshold})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81afb38",
   "metadata": {},
   "source": [
    "#### Extract Relationships Around Pivots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afb7d0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧩 Extracting sub-relations around pivots...\n",
      "✅ 616,008 relationships extracted\n"
     ]
    }
   ],
   "source": [
    "# 🧩 Extract relationships connected to the pivot nodes\n",
    "print(\"🧩 Extracting sub-relations around pivots...\")\n",
    "\n",
    "sub_rels = relationships[\n",
    "    relationships[\"node_id_start\"].isin(top_nodes[\"node_id\"]) |\n",
    "    relationships[\"node_id_end\"].isin(top_nodes[\"node_id\"])\n",
    "]\n",
    "\n",
    "print(f\"✅ {len(sub_rels):,} relationships extracted\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d701d7b",
   "metadata": {},
   "source": [
    "#### Extract corresponding nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ade1048c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Extracting related nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading nodes: 100%|██████████| 4/4 [00:24<00:00,  6.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 463,389 nodes extracted\n"
     ]
    }
   ],
   "source": [
    "# 📦 Extract nodes connected to the selected relationships\n",
    "print(\"📦 Extracting related nodes...\")\n",
    "\n",
    "# Récupérer tous les node_id présents dans les relations extraites\n",
    "node_ids = pd.unique(sub_rels[[\"node_id_start\", \"node_id_end\"]].values.ravel())\n",
    "\n",
    "all_nodes_list = []\n",
    "\n",
    "# Charger les fichiers de nœuds et ne garder que ceux correspondant aux node_ids\n",
    "for file in tqdm(NODE_FILES, desc=\"Loading nodes\"):\n",
    "    if os.path.exists(file):\n",
    "        df = pd.read_csv(file, low_memory=False)\n",
    "        df = df[df[\"node_id\"].isin(node_ids)]\n",
    "        all_nodes_list.append(df)\n",
    "\n",
    "# Combiner tous les nœuds extraits en un seul DataFrame\n",
    "sub_nodes = pd.concat(all_nodes_list, ignore_index=True)\n",
    "\n",
    "print(f\"✅ {len(sub_nodes):,} nodes extracted\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d0986",
   "metadata": {},
   "source": [
    "#### Add node type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2233ca18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ashahi\\AppData\\Local\\Temp\\ipykernel_24464\\1968801901.py:4: DtypeWarning: Columns (2,3,6,7,10,11,12,13,14,15,16,17,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  sub_nodes.loc[sub_nodes[\"node_id\"].isin(pd.read_csv(file)[\"node_id\"]), \"node_type\"] = \"Entity\"\n"
     ]
    }
   ],
   "source": [
    "sub_nodes[\"node_type\"] = None\n",
    "for file in NODE_FILES:\n",
    "    if \"entities\" in file:\n",
    "        sub_nodes.loc[sub_nodes[\"node_id\"].isin(pd.read_csv(file)[\"node_id\"]), \"node_type\"] = \"Entity\"\n",
    "    elif \"intermediaries\" in file:\n",
    "        sub_nodes.loc[sub_nodes[\"node_id\"].isin(pd.read_csv(file)[\"node_id\"]), \"node_type\"] = \"Intermediary\"\n",
    "    elif \"officers\" in file:\n",
    "        sub_nodes.loc[sub_nodes[\"node_id\"].isin(pd.read_csv(file)[\"node_id\"]), \"node_type\"] = \"Officer\"\n",
    "    elif \"addresses\" in file:\n",
    "        sub_nodes.loc[sub_nodes[\"node_id\"].isin(pd.read_csv(file)[\"node_id\"]), \"node_type\"] = \"Address\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0e2f12",
   "metadata": {},
   "source": [
    "#### Save extracted nodes and relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5de39b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎉 Mini-Dense Extraction completed!\n",
      "👉 Nodes saved to: ./extracts_mini_dense/nodes_extracted.csv\n",
      "👉 Relationships saved to: ./extracts_mini_dense/relations_extracted.csv\n"
     ]
    }
   ],
   "source": [
    "sub_rels.to_csv(os.path.join(OUTPUT_DIR, \"relations_extracted.csv\"), index=False)\n",
    "sub_nodes.to_csv(os.path.join(OUTPUT_DIR, \"nodes_extracted.csv\"), index=False)\n",
    "\n",
    "print(\"\\n🎉 Mini-Dense Extraction completed!\")\n",
    "print(f\"👉 Nodes saved to: {OUTPUT_DIR}/nodes_extracted.csv\")\n",
    "print(f\"👉 Relationships saved to: {OUTPUT_DIR}/relations_extracted.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2ecae0",
   "metadata": {},
   "source": [
    "### Dense Subgraph Refinement – Filtering and Top Cluster Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda7286b",
   "metadata": {},
   "source": [
    "#### Library Imports and Basic Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a44a8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading files...\n",
      "✅ 463,389 nodes | 616,008 relationships\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Folder where the extracted files are located\n",
    "extract_dir = \"./extracts_mini_dense\"\n",
    "\n",
    "# Load CSVs\n",
    "print(\"📂 Loading files...\")\n",
    "nodes = pd.read_csv(f\"{extract_dir}/nodes_extracted.csv\", low_memory=False)\n",
    "rels = pd.read_csv(f\"{extract_dir}/relations_extracted.csv\", low_memory=False)\n",
    "print(f\"✅ {len(nodes):,} nodes | {len(rels):,} relationships\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec705120",
   "metadata": {},
   "source": [
    "#### Calculate Node Degree (number of connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b924842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate degree\n",
    "degree = pd.concat([rels['node_id_start'], rels['node_id_end']]).value_counts()\n",
    "degree = degree.rename_axis('node_id').reset_index(name='degree')\n",
    "\n",
    "# Remove the 'degree' column if it already exists to avoid MergeError\n",
    "if 'degree' in nodes.columns:\n",
    "    nodes = nodes.drop(columns=['degree'])\n",
    "\n",
    "# Merge with nodes\n",
    "nodes = nodes.merge(degree, on='node_id', how='left')\n",
    "\n",
    "# Replace NaN with 0\n",
    "nodes['degree'] = nodes['degree'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2f9561",
   "metadata": {},
   "source": [
    "#### Filter “interesting” nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "397a4eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Filtered nodes: 93,695\n",
      "✅ Filtered relationships: 244,795\n"
     ]
    }
   ],
   "source": [
    "filtered_nodes = nodes[\n",
    "    ((nodes[\"node_type\"] == \"Intermediary\") & (nodes[\"degree\"] > 50)) |\n",
    "    ((nodes[\"node_type\"] == \"Officer\") & (nodes[\"degree\"] > 2)) |\n",
    "    ((nodes[\"node_type\"] == \"Address\") & (nodes[\"degree\"] > 3)) |\n",
    "    ((nodes[\"node_type\"] == \"Entity\") & (nodes[\"degree\"] > 1))\n",
    "].copy()\n",
    "\n",
    "# ✅ Use node_id instead of id\n",
    "filtered_ids = set(filtered_nodes[\"node_id\"])\n",
    "\n",
    "filtered_rels = rels[\n",
    "    rels[\"node_id_start\"].isin(filtered_ids) &\n",
    "    rels[\"node_id_end\"].isin(filtered_ids)\n",
    "].copy()\n",
    "\n",
    "print(f\"✅ Filtered nodes: {len(filtered_nodes):,}\")\n",
    "print(f\"✅ Filtered relationships: {len(filtered_rels):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f9bccd",
   "metadata": {},
   "source": [
    "#### Detect the densest clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7e223ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Number of subgraphs: 21\n",
      "🔹 Size of the largest cluster: 57166\n",
      "🏆 Final graph: 82,012 nodes | 218,457 relationships\n"
     ]
    }
   ],
   "source": [
    "G = nx.from_pandas_edgelist(filtered_rels, \"node_id_start\", \"node_id_end\")\n",
    "components = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "\n",
    "print(f\"🔹 Number of subgraphs: {len(components)}\")\n",
    "print(f\"🔹 Size of the largest cluster: {len(components[0])}\")\n",
    "\n",
    "# Keep the 5 largest clusters\n",
    "top_components = components[:5]\n",
    "top_nodes = set().union(*top_components)\n",
    "\n",
    "# ✅ Corrected: node_id instead of id\n",
    "dense_nodes = filtered_nodes[filtered_nodes[\"node_id\"].isin(top_nodes)].copy()\n",
    "dense_rels = filtered_rels[\n",
    "    filtered_rels[\"node_id_start\"].isin(top_nodes) &\n",
    "    filtered_rels[\"node_id_end\"].isin(top_nodes)\n",
    "].copy()\n",
    "\n",
    "print(f\"🏆 Final graph: {len(dense_nodes):,} nodes | {len(dense_rels):,} relationships\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af16c28",
   "metadata": {},
   "source": [
    "#### Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "337e017b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Files saved in: ./extracts_mini_dense/filtered\n",
      "✅ Reduction successfully completed!\n"
     ]
    }
   ],
   "source": [
    "output_dir = f\"{extract_dir}/filtered\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "dense_nodes.to_csv(f\"{output_dir}/nodes_interesting.csv\", index=False)\n",
    "dense_rels.to_csv(f\"{output_dir}/relationships_interesting.csv\", index=False)\n",
    "\n",
    "print(\"💾 Files saved in:\", output_dir)\n",
    "print(\"✅ Reduction successfully completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98afc441",
   "metadata": {},
   "source": [
    "### Ultra-Targeted Mini-Graph Extraction – Top Hubs and Direct Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70936a6a",
   "metadata": {},
   "source": [
    "#### Basic configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9205260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "extract_dir = \"./extracts_mini_dense/filtered\"\n",
    "output_dir = f\"{extract_dir}/mini_graph\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load filtered files\n",
    "nodes = pd.read_csv(f\"{extract_dir}/nodes_interesting.csv\", low_memory=False)\n",
    "rels = pd.read_csv(f\"{extract_dir}/relationships_interesting.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bfba8c",
   "metadata": {},
   "source": [
    "#### Select top hubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e20398a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 500  # number of hubs to keep\n",
    "top_hubs = nodes.sort_values(\"degree\", ascending=False).head(top_n)\n",
    "top_hub_ids = set(top_hubs[\"node_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f17bb4",
   "metadata": {},
   "source": [
    "#### Extract direct neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d97d32e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_start = rels[rels[\"node_id_start\"].isin(top_hub_ids)][\"node_id_end\"]\n",
    "neighbors_end = rels[rels[\"node_id_end\"].isin(top_hub_ids)][\"node_id_start\"]\n",
    "\n",
    "neighbor_ids = set(neighbors_start).union(set(neighbors_end))\n",
    "\n",
    "# All nodes of the mini-graph\n",
    "mini_node_ids = top_hub_ids.union(neighbor_ids)\n",
    "mini_nodes = nodes[nodes[\"node_id\"].isin(mini_node_ids)]\n",
    "\n",
    "# Relationships of the mini-graph\n",
    "mini_rels = rels[\n",
    "    rels[\"node_id_start\"].isin(mini_node_ids) &\n",
    "    rels[\"node_id_end\"].isin(mini_node_ids)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33307f22",
   "metadata": {},
   "source": [
    "#### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89ad4c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ultra-targeted mini-graph created!\n",
      "📌 Nodes: 82,012\n",
      "📌 Relationships: 218,457\n",
      "💾 Files saved in: ./extracts_mini_dense/filtered/mini_graph\n"
     ]
    }
   ],
   "source": [
    "mini_nodes.to_csv(f\"{output_dir}/nodes_mini.csv\", index=False)\n",
    "mini_rels.to_csv(f\"{output_dir}/relationships_mini.csv\", index=False)\n",
    "\n",
    "print(\"✅ Ultra-targeted mini-graph created!\")\n",
    "print(f\"📌 Nodes: {len(mini_nodes):,}\")\n",
    "print(f\"📌 Relationships: {len(mini_rels):,}\")\n",
    "print(f\"💾 Files saved in: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75abb8e7",
   "metadata": {},
   "source": [
    "### Final Subgraph Extraction & Metrics – Panama Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e526fcc8",
   "metadata": {},
   "source": [
    "#### Loading the Mini-Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "179affca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Number of nodes: 82012\n",
      "🔹 Number of edges: 199012\n",
      "🔹 Number of connected components: 5\n",
      "🔹 Size of the largest cluster: 57166\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "output_dir = \"./extracts_mini_dense/filtered/mini_graph\"\n",
    "\n",
    "# Load exported files\n",
    "nodes = pd.read_csv(f\"{output_dir}/nodes_mini.csv\", low_memory=False)\n",
    "rels = pd.read_csv(f\"{output_dir}/relationships_mini.csv\", low_memory=False)\n",
    "\n",
    "# Create the graph\n",
    "G = nx.from_pandas_edgelist(rels, \"node_id_start\", \"node_id_end\")\n",
    "\n",
    "print(\"🔹 Number of nodes:\", G.number_of_nodes())\n",
    "print(\"🔹 Number of edges:\", G.number_of_edges())\n",
    "print(\"🔹 Number of connected components:\", nx.number_connected_components(G))\n",
    "\n",
    "# Largest cluster\n",
    "largest_cc = max(nx.connected_components(G), key=len)\n",
    "print(\"🔹 Size of the largest cluster:\", len(largest_cc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2c8d72",
   "metadata": {},
   "source": [
    "#### Centrality Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e9af4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        name     node_type  \\\n",
      "81985                                    NaN       Address   \n",
      "81906                Standard Directors Ltd.       Officer   \n",
      "81907                        Execorp Limited       Officer   \n",
      "49007              White Willow Trading Ltd.        Entity   \n",
      "81914      Portcullis TrustNet (BVI) Limited       Officer   \n",
      "81893  Christabel Corporate Services Limited  Intermediary   \n",
      "81988                                    NaN       Address   \n",
      "50835                 Wenbart Alliance Corp.        Entity   \n",
      "53686               TRUDELT PRODUCTIONS INC.        Entity   \n",
      "51272              SEPROTECH INVESTMENT INC.        Entity   \n",
      "\n",
      "       betweenness_centrality  \n",
      "81985                0.286665  \n",
      "81906                0.205297  \n",
      "81907                0.191670  \n",
      "49007                0.190932  \n",
      "81914                0.123408  \n",
      "81893                0.097233  \n",
      "81988                0.097233  \n",
      "50835                0.061448  \n",
      "53686                0.061448  \n",
      "51272                0.061448  \n"
     ]
    }
   ],
   "source": [
    "degree_centrality = nx.degree_centrality(G)\n",
    "betweenness_centrality = nx.betweenness_centrality(G, k=2000)  # sample for speed\n",
    "\n",
    "# Add to nodes dataframe\n",
    "nodes[\"degree_centrality\"] = nodes[\"node_id\"].map(degree_centrality)\n",
    "nodes[\"betweenness_centrality\"] = nodes[\"node_id\"].map(betweenness_centrality)\n",
    "\n",
    "# Top 10 pivots by betweenness centrality\n",
    "top_pivots = nodes.sort_values(\"betweenness_centrality\", ascending=False).head(10)\n",
    "print(top_pivots[[\"name\", \"node_type\", \"betweenness_centrality\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d828f3c",
   "metadata": {},
   "source": [
    "#### Extract Narrative Subgraphs – Panama Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f83690e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Building main graph...\n",
      "📊 Global graph: 82,012 nodes | 199,012 edges\n"
     ]
    }
   ],
   "source": [
    "# Output folder\n",
    "output_dir = \"./subgraphs_narratives\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"🔍 Building main graph...\")\n",
    "G = nx.from_pandas_edgelist(rels, \"node_id_start\", \"node_id_end\")\n",
    "G = G.subgraph(nodes[\"node_id\"]).copy()  # ✅ correction\n",
    "\n",
    "# Add node attributes from nodes dataframe\n",
    "attrs = nodes.set_index(\"node_id\").to_dict(orient=\"index\")\n",
    "nx.set_node_attributes(G, attrs)\n",
    "\n",
    "# Check\n",
    "print(f\"📊 Global graph: {len(G):,} nodes | {G.number_of_edges():,} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5eebfc",
   "metadata": {},
   "source": [
    "#### Extract: Central Intermediaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc2d9cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 'Intermediaries' subgraph: 800 nodes, 5348 edges\n"
     ]
    }
   ],
   "source": [
    "# Select top central intermediaries\n",
    "interm_nodes = nodes[nodes[\"node_type\"] == \"Intermediary\"].nlargest(10, \"degree_centrality\")[\"node_id\"]\n",
    "\n",
    "# Build combined subgraph (radius=2 for neighbors)\n",
    "sub1 = nx.ego_graph(G, interm_nodes.iloc[0], radius=2)\n",
    "for n in interm_nodes[1:]:\n",
    "    sub1 = nx.compose(sub1, nx.ego_graph(G, n, radius=2))\n",
    "\n",
    "# Keep top 800 most connected nodes\n",
    "degrees = dict(sub1.degree())\n",
    "top_nodes = sorted(degrees, key=degrees.get, reverse=True)[:800]\n",
    "sub1 = sub1.subgraph(top_nodes).copy()\n",
    "\n",
    "print(f\"📊 'Intermediaries' subgraph: {len(sub1)} nodes, {sub1.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6079cfaa",
   "metadata": {},
   "source": [
    "#### Pivot Officers Subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62596fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 'Pivot Officers' subgraph: 700 nodes, 3279 edges\n"
     ]
    }
   ],
   "source": [
    "# Select top connected officers\n",
    "officers = nodes[nodes[\"node_type\"] == \"Officer\"].nlargest(10, \"degree_centrality\")[\"node_id\"]\n",
    "\n",
    "# Build combined subgraph (merge ego_graphs)\n",
    "sub3 = nx.ego_graph(G, officers.iloc[0], radius=2)\n",
    "for o in officers[1:]:\n",
    "    sub3 = nx.compose(sub3, nx.ego_graph(G, o, radius=2))\n",
    "\n",
    "# Keep top 700 most connected nodes\n",
    "degrees = dict(sub3.degree())\n",
    "top_nodes = sorted(degrees, key=degrees.get, reverse=True)[:700]\n",
    "sub3 = sub3.subgraph(top_nodes).copy()\n",
    "\n",
    "print(f\"📊 'Pivot Officers' subgraph: {len(sub3)} nodes, {sub3.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2e343a",
   "metadata": {},
   "source": [
    "#### Export CSV for all subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a3b260d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Subgraph 'subgraph_1_intermediaries' exported: 800 nodes, 5348 edges\n",
      "✅ Subgraph 'subgraph_3_officers' exported: 700 nodes, 3279 edges\n"
     ]
    }
   ],
   "source": [
    "def export_subgraph(subgraph, filename_base):\n",
    "    \"\"\"\n",
    "    Export a NetworkX subgraph to CSV and GEXF\n",
    "    \"\"\"\n",
    "    # Nodes with attributes\n",
    "    nodes_df = pd.DataFrame.from_dict(dict(subgraph.nodes(data=True)), orient='index').reset_index()\n",
    "    nodes_df.rename(columns={'index': 'node_id'}, inplace=True)\n",
    "    \n",
    "    # Edges / relationships\n",
    "    edges_df = nx.to_pandas_edgelist(subgraph)\n",
    "    \n",
    "    # Export CSV\n",
    "    nodes_df.to_csv(f\"{output_dir}/{filename_base}_nodes.csv\", index=False)\n",
    "    edges_df.to_csv(f\"{output_dir}/{filename_base}_edges.csv\", index=False)\n",
    "    \n",
    "    print(f\"✅ Subgraph '{filename_base}' exported: {len(nodes_df)} nodes, {len(edges_df)} edges\")\n",
    "\n",
    "# Exports\n",
    "export_subgraph(sub1, \"subgraph_1_intermediaries\")\n",
    "export_subgraph(sub3, \"subgraph_3_officers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18383a83",
   "metadata": {},
   "source": [
    "#### Basic Statistics of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9789e1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Number of nodes: 82012\n",
      "📊 Number of edges: 199012\n",
      "📊 Number of connected components: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"📊 Number of nodes:\", G.number_of_nodes())\n",
    "print(\"📊 Number of edges:\", G.number_of_edges())\n",
    "print(\"📊 Number of connected components:\", nx.number_connected_components(G))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
